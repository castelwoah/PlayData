2024.04.01
CLI - Command Line Interface 
 
 - Port  22 
  - telnet 
  -> ssh 

GUI - Graphic User Interface 


# 디렉토리 변경 
# cd 
cd ~ 

# pwd 
# 현재 폴더 위치 

# 폴더 생성 
mkdir workspace 


sudo apt update
sudo apt install python3-pip 
sudo apt install python3-pip  -y

# jupyter 설치하기 
pip install jupyter 

# vs code 설치하기 
https://code.visualstudio.com/docs/?dv=win64user

# vs code 실행 
extensions -> wsl 설치 

f1 -> connect to wsl 





text3 = "오늘도 아침엔 입에 빵을 물고"

text4 = """오늘도 아침엔 입에 빵을 물고
똑같이 하루를 시작하고
온종일 한 손엔 아이스 아메리카노
피곤해 죽겠네

지하철 속 이 장면 어제 꿈에서 봤나
아참 매일이지 지나치고
바쁜 이 삶에 그냥 흔한 날에
그 애를 보고 말야

평온했던 하늘이 무너지고
어둡던 눈앞이 붉어지며
뭔가 잊고 온 게 있는 것 같아
괜히 이상하게 막 울 것만 같고
그냥 지나치는 게 나을 것 같아
나는 생각은 딱 질색이니까

카페인으로 잡은 정신은 빠졌고
하루 종일 신경 쓰여 토할 것 같아
저녁이 돼도 배고픔까지 까먹고
그치 이상하지 근데 말야 있잖아

처음 본 순간 뭐라 할까 그립달까
나도 웃긴데 말야

평온했던 하늘이 무너지고
어둡던 눈앞이 붉어지며
뭔가 잊고 온 게 있는 것 같아
괜히 이상하게 막 울 것만 같고
그냥 지나치는 게 나을 것 같아
나는 생각은 딱 질색이니까

오랫동안 나를 아는
슬픈 표정을 하고 Oh
흔적 없는 기억 밖
혹 과거에 미래에 딴 차원에 세계에
1 2 3 4 5 6 7 8

평온했던 하늘이 무너지고
어둡던 눈앞이 붉어져도
다시 놓쳐버리는 것만 같아
괜히 이상하게 막 울 것만 같고
그냥 지나치는 게 나을 것 같아
나는 생각은 딱 질색이니까

아냐 지나치는 게 나을 것 같아
나는 아픈 건 딱 질색이니까"""



loan = "김미영팀장입니다. {}님께서는 최저이율로 최고 3000만원가지 입금 가능합니다."

name = ["권시은","기석광","김가현","김기호","김대건","김동현","김승주","김예송","김하영","김현지","노석현","박성우","변수현","신소영","원정인","유선우","유정연","이서연","이재연","이충원","이하은","이희재","정다인","조명아","조태식","조혜민","차민혁","최성현","최태성","한다솜",]



import requests
url = "https://www.starbucks.co.kr/store/getStore.do?r=EXRPGE2OU7"
payload = {"in_biz_cds" : "0","in_scodes" : "0","ins_lat" : "37.4947","ins_lng" : "127.0493","search_text" : "","p_sido_cd" : "01","p_gugun_cd" : "","isError" : "true","in_distance" : "0","in_biz_cd" : "","iend" : "1000","searchType" : "C","set_date" : "","rndCod" : "Q878EXUG04","all_store" : "0","T03" : "0","T01" : "0","T27" : "0","T12" : "0","T09" : "0","T30" : "0","T05" : "0","T22" : "0","T21" : "0","T10" : "0","T36" : "0","T43" : "0","T48" : "0","Z9999" : "0","P02" : "0","P10" : "0","P50" : "0","P20" : "0","P60" : "0","P30" : "0","P70" : "0","P40" : "0","P80" : "0","whcroad_yn" : "0","P90" : "0","P01" : "0","new_bool" : "0",}
r = requests.post(url, data=payload)
data = r.json()


#int, #float, #list, #for #if~elif~else


내일 학습 : 
# list comprehension
# dict
# 사용자 함수
# lambda 
# break
# continue 
# 데이터 수집 
# 워드 카운트




2024.04.02

DNS 
Domain Name Server 

TCP/ IP

Protocol -> 약속 


IP -> 



jupyter notebook --generate-config



cd ~ 
pwd


# 목록 보기 
ls 
# 목록 보기 - 상세 
ls -al

drwxr-xr-x 1 gen2 gen2 4096 Apr  2 09:48 .jupyter

drwxr-xr-x 
	- d : directory  , - : file

 rwx        r-x      r-x 
내꺼     그룹    제3자 
r: read, w: write, x: execute 


echo "hi" > a.txt
chmod 646 a.txt 

other 
chmod o-w a.txt
chmod g+w a.txt
chmod u-w a.txt


cd .jupyter 

vim


비주얼 모드 
i 버튼 -> 쓰기 모드 

: -> 명령어 모드 


강제로 탈출 -> q! 

쓰기 w 

쓰고 나가기 wq 


ipython
from jupyter_server.auth import passwd

passwd()

vim jupyter_notebook_config.py


# password 키워드  찾기 
:/password 

n : 다음 찾기 
N : 이전 찾기 






:/notebook_dir 


c.ServerApp.notebook_dir = ""
c.ServerApp.password = ""

# 쓰고 나가기 
:/wq 


empty_list =[]
for x in range(0,10):
    if x % 2 == 0:
        empty_list.append(x)

# list comprehension(리스트 축약형)

empty_list2 = [x for x in range(0, 10) if x % 2 ==0]



from datetime import date, datetime, timedelta

for t in range(0, 1001, 100):
    print(date.today() + timedelta(days=t))

master = dict(zip(range(0,7), "월화수목금토일"))

from datetime import date, datetime, timedelta

for t in range(0, 1001, 100):
    # f-string 3.6이상 
    print(f"{t}일째 - {date.today() + timedelta(days=t)} - {master[(date.today() + timedelta(days=t)).weekday()]}")



# 요일 키 추가 
for star in data['list']:
    star['요일'] = master[datetime.strptime(star['open_dt'], "%Y%m%d").weekday()]


#cnt_dict = {}
cnt_dict = {}
for x in data['list']:
    # print(x['요일'])
    if x['요일'] in cnt_dict:
        #cnt_dict[x['요일']] = cnt_dict[x['요일']] + 1 
        cnt_dict[x['요일']] +=  1 
    else:
        cnt_dict[x['요일']] = 1



[x['s_name']  for x in data['list'] if x['s_name'][-2:] == "DT"]

[x['addr'].split()[1] for x in data['list']]



len(list(set(([x['addr'].split()[1] for x in data['list']]))))



list_data = ['1', '2', '3','4', '5', '6', '7', '8']
for x in list_data:
    print(int(x) ** 2)


def tmp(x):
    return int(x) ** 2
list(map(tmp, list_data))


url = "https://www.genie.co.kr/chart/top200"


head = {'User-Agent':
'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0'}

r = requests.get(url, headers=head)

r.text 


from bs4 import BeautifulSoup
bs = BeautifulSoup(r.text)



for idx, x in enumerate(bs.findAll("td",class_='info')):
    title = x.find("a", class_="title ellipsis").text.strip()
    artist = x.find("a", class_="artist ellipsis").text
    print(f"{idx+1}위 -> {title} - {artist}")


lyric = "https://www.genie.co.kr/detail/songInfo?xgnm={}"
for idx, x in enumerate(bs.findAll("td",class_='info')):
    title = x.find("a", class_="title ellipsis").text.strip()
    artist = x.find("a", class_="artist ellipsis").text
    id_ = x.find("a", class_="title ellipsis")['onclick'].split("'")[1]
    print(f"{idx+1}위 -> {title} - {artist} ")
    print(lyric.format(id_))


r2 = requests.get("https://www.genie.co.kr/detail/songInfo?xgnm=105544013", headers=head)

bs2 = BeautifulSoup(r2.text)
# 상대경로 -> ./ -> 현재 폴더 
# ../ -> 현재 경로에서 상위 폴더 
# 절대경로 
f = open('./a.txt', "w", encoding='utf-8')
f.write(bs2.find("pre", id="pLyrics").text.strip())
f.close()

target = "./lyric"
if not os.path.isdir(target):
    os.mkdir(target)
1

# 50위까지 노래 가사 저장하기 
저장 위치 -> ./lyric
# 파일 형식 -> 가수_제목.txt 







2024.04.03

# 프로세스 확인 
ps 
## 모든 프로세스 확인 
ps -ef 

## | 
## grep 패턴 
ps -ef | grep python

# 프로세스 kill
kill -9 [pid]




cd ~/workspace



# shell
-> bash 


ls -al ~/


# ssh 설치 
sudo apt update 


sudo apt install openssh-server 



sudo service ssh start

sudo service ssh status



# 가상환경 만들기 
pip install virtualenv 

python3 -m virtualenv venv 

ls -al 

python3 -m virtualenv venv


# pip 설치 방법 
pip install pandas 

pip install pandas==1.2.0

pip uninstall pandas 

# 패키지 버전 확인 
pip list 
pip freeze 


# requirements.txt 
pip install -r requirements.txt


# nohup -> session 유지 
# & -> background 
nohup jupyter-notebook &





import os
from tqdm import tqdm
lyrics_url = "https://www.genie.co.kr/detail/songInfo?xgnm={}"
if not os.path.isdir("./lyrics"):
    os.mkdir("./lyrics")
for x in tqdm(bs.find("div", {'class': \
                          "music-list-wrap"}).findAll("td", {"class" : "info"})):
    
    title = x.find("a", class_="title ellipsis").text.strip()
    artist = x.find("a", class_="artist ellipsis").text
    f = open(f"./lyrics/{artist}_{title}.txt", "w", encoding='utf-8')
    #print(f"{artist}-{title}.txt")
    r3 = requests.get(lyrics_url.format(p.findall(str(x))[0]),headers=head)
    f.write(BeautifulSoup(r3.text).find("pre", id="pLyrics").text.strip())
    f.close()




# linux 

# 이동 및 이름 바꾸기 
-> mv [원본] [타켓] 

# 삭제 
rm 
## 폴더 삭제 
rm -rf [폴더] 

# 복사 
cp 
# 하위폴더까지 복사 
cp -R [대상]



https://finance.naver.com/


url = "https://m.stock.naver.com/front-api/v1/external/chart/domestic/info?symbol={}&requestType=1&startTime={}&endTime={}&timeframe=day"


r = requests.get(url.format('005930', '20230102', '20240402'))


import csv 
f = open("./삼성전자.csv", 'w', newline='')
write_stock = csv.writer(f)
write_stock.writerows(data)
f.close()


# 아래 주소에서 api를 찾아서 데이터를 가져오고 
# kospi 만 
# 네이버 주식 api를 이용해서 
# stock폴더를 만들고 그 안에 종목이름.csv 
# 네이버에서 데이터 가져와서 저장하는 기능을 사용자 함수로 만들것
def get_naver_stock(code, start_date, end_date):
    """
    code -> 종목 코드 넣어 
    start_date -> 시작 날짜 
    end_date -> 끝나는 날짜
    """
    pass


http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020201





2024.04.04
# 인터넷에 있는 파일 다운로드 받을때 사용 wget 
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb



public ip 
private ip 



sudo apt install ./google-chrome-stable_current_amd64.deb -y




# 일반계정 
nohup jupyter-notebook --ip=0.0.0.0 & 

# root
nohup jupyter-notebook --allow-root --ip=0.0.0.0 & 


# 네트워크 프로그램 설치 
sudo apt install net-tools 



# port 
sudo netstat -ntlp | grep [포트] 

# 실행되고 있는 실행파일의 위치 
which 
which google-chrome


cd /opt/google/

pip install webdriver_manager



from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
driver = webdriver.Chrome(service=Service(ChromeDriverManager(driver_version="123.0.6312.105").install()))

driver.get("https://www.koreabaseball.com/")

from selenium.webdriver.common.by import By
driver.find_element(By.CSS_SELECTOR, "#lnb > li:nth-child(3) > a").click()
driver.page_source




import time
import re
pattern = re.compile("playerId=([0-9]+)")

select_page = "#cphContents_cphContents_cphContents_ucPager_btnNo{}"
select_team = "#cphContents_cphContents_cphContents_ddlTeam > option:nth-child({})"
playid = []
for x in range(2,12):
    for_1 = select_team.format(x)
    driver.find_element(By.CSS_SELECTOR, for_1).click()
    time.sleep(2)
    #playid.extend(pattern.findall(driver.page_source))
    for y in range(1,6):
        f2 = select_page.format(y)
        try:
            driver.find_element(By.CSS_SELECTOR, f2).click()
            time.sleep(1)
            playid.extend(pattern.findall(driver.page_source))
        except Exception as e:
            print ("page 없음 ")
        time.sleep(2)





import pickle 
# binary save , load 
with open("./kbo.pkl", "wb") as f:
    pickle.dump(playid, f)


import pickle
with open("./kbo.pkl", "rb") as f:
    abc = pickle.load(f)






#lnb > li:nth-child(3) > a




https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe



# conda 가상환경 
conda create --name [이름] python=[버전] 


conda install jupyter 
pip install selenium webdriver_manager



play_url = "https://www.koreabaseball.com/Record/Player/PitcherDetail/Basic.aspx?playerId={}"
from bs4 import BeautifulSoup as BS 
import requests 


def get_kbo(id_):
    kbo_r = requests.get(play_url.format(id_))
    bs = BS(kbo_r.text)
    data = {x.text.split(":")[0] : x.text.split(":")[1] for x in bs.find("div", class_= "player_basic").findAll("li")}
    data['team'] = bs.find("h4", id="h4Team").text
    return data





2024.04.05

import requests 

payload = {"ajax": "true",
"curCd": "",
"tmpInqStrDt": "2024-04-03",
"pbldDvCd": "0",
"pbldSqn": "",
"hid_key_data": "",
"inqStrDt": "20240403",
"inqKindCd": "1",
"hid_enc_data": "",
"requestTarget": "searchContentDiv",}

url = "https://www.kebhana.com/cms/rate/wpfxd651_01i_01.do"

r = requests.post(url, data=payload)

hana = pd.read_html(r.text)[0]



def get_exchange(code_, date_=None):
    return hana[hana['통화_통화'].str.find(f"{code_}") > -1]





def get_exchange(code_="USD", date_=None):
    """
     code_ = 통화코드 
         예) USD 
     date_ = 예) '2024-01-02'
    """
    payload = {"ajax": "true",
            "curCd": "",
            "pbldDvCd": "0",
            "pbldSqn": "",
            "hid_key_data": "",
            "inqKindCd": "1",
            "hid_enc_data": "",
            "requestTarget": "searchContentDiv",}
    payload['tmpInqStrDt'] = date_
    payload['inqStrDt'] = date_.replace("-", "")
    url = "https://www.kebhana.com/cms/rate/wpfxd651_01i_01.do"
    r = requests.post(url, data=payload)
    exchange = pd.read_html(r.text)[0]
    exchange.columns = ["_".join(x[:2]) for x in exchange.columns]
    return exchange.loc[exchange['통화_통화'].str.find(f"{code_.upper()}") > -1, '현찰_파실 때'].iloc[0,0]



----

from datetime import date, datetime
import requests 
import pandas as pd 

def get_exchange(code_="USD", date_=None):
    """
     code_ = 통화코드 
         예) USD 
     date_ = 예) '2024-01-02'
    """
    payload = {"ajax": "true",
            "curCd": "",
            "pbldDvCd": "0",
            "pbldSqn": "",
            "hid_key_data": "",
            "inqKindCd": "1",
            "hid_enc_data": "",
            "requestTarget": "searchContentDiv",}
    # 날짜 형식 검사
    try:
        datetime.strptime(date_, "%Y-%m-%d")
    except:
        print ("값 확인해!!")
        return -1
        
    
    payload['tmpInqStrDt'] = date_
    payload['inqStrDt'] = date_.replace("-", "")
    url = "https://www.kebhana.com/cms/rate/wpfxd651_01i_01.do"
    r = requests.post(url, data=payload)
    exchange = pd.read_html(r.text)[0]
    exchange.columns = ["_".join(x[:2]) for x in exchange.columns]
    return exchange.loc[exchange['통화_통화'].str.find(f"{code_.upper()}") > -1, '현찰_파실 때'].iloc[0,0]





f = open("./file.txt", "w", encoding="utf-8")
for roots, dirs, files in os.walk("/home/gen2"):
    for file in files:
        f.write(f"{roots}/{file}\n")
f.close()




# dict comprehension 
code_master = {x1.split()[0] : x1.split()[1] for x1 in hana['통화_통화']}



kbo2 = kbo_df[kbo_df.연봉.str.find("달러") > -1].copy()



# warning 숨기기 
import warnings
warnings.filterwarnings(action='ignore')


exchange =  get_exchange('usd', '2024-01-02')

kbo2['연봉'] = kbo2.연봉.apply(lambda x : int(x[:-2]) * exchange) / 10000



kbo3 = kbo_df[~kbo_df.연봉.str.find("달러") > -1].copy()


# and -> % 
# or -> | 
# not -> ~



kbo4 = kbo3[~kbo3.연봉.apply(lambda x : len(x) < 3)].copy()

kbo4.연봉 = kbo4.연봉.apply(lambda x : int(x[:-2]))


final = pd.concat([kbo2, kbo4])


final.groupby(['team'])[['연봉']].mean().sort_values(by=['연봉'], ascending=False)



final.groupby(['team'])[['연봉']].agg(['mean', 'median', 'var'])




공개키 (public key) 
개인키(private key)


sudo apt install unzip 
unzip ./이름_생년_성별_10000.zip -d ./data


파일 분리하여 저장 
남성 -> 남성 폴더에 저장 
여성 -> 여성 폴더에 저장 

'1', '3' 
'2', '4' 

남성, 여성 컬럼으로 DataFrame으로 
.to_csv("./report.csv", index=False, encoding='utf-8-sig')


# 파일 복사 
import shutil
shutil.move("./result.csv", "/home/gen2")







2024.04.08
kbo['초등'] = kbo['경력'].apply(lambda x : x.split("-")[0])



kbo['생년월일'].apply(lambda x : datetime.strptime(x.strip(), "%Y년 %m월 %d일"))

kbo['생년월일'] = kbo['생년월일'].apply(lambda x : datetime.strptime(x.strip(), "%Y년 %m월 %d일"))

kbo['생존일'] = kbo['생년월일'].apply(lambda x : (datetime.now() - x).days)

kbo.sort_values(by=['생존일', '선수명'], ascending=[False, True])


kbo['나이'] = kbo['생년월일'].apply(lambda x : datetime.now().year - x.year)


kbo.loc[kbo.team == "고양 히어로즈", 'team']  = '키움 히어로즈'



매장/서비스 > 매장검색 | 대한민국 대표편의점 GS25 (gsretail.com)



import requests 
from bs4 import BeautifulSoup as BS 

r= requests.get("http://gs25.gsretail.com/gscvs/ko/store-services/locations")

bs = BS(r.text)

#<form id="CSRFForm" action="/gscvs/ko/store-services/locations" method="post"><input type="hidden" name="CSRFToken" value="406690e8-0340-4931-b372-4b5519577c3c" />
bs.find("input", {'name': "CSRFToken"})



with requests.Session() as s:
    r = s.get("http://gs25.gsretail.com/gscvs/ko/store-services/locations")
    bs = BS(r.text)
    token = bs.find("input", {'name': "CSRFToken"})
    print(bs.find("input", {'name': "CSRFToken"}))
    target = f"http://gs25.gsretail.com/gscvs/ko/store-services/locationList?CSRFToken={token}"
    





payload = {"pageNum": "1",
"pageSize": "5",
"searchShopName": "",
"searchSido": "26",
"searchGugun": "2671",
"searchDong": "26710310",
"searchType": "",
"searchTypeService": "0",
"searchTypeToto": "0",
"searchTypeCafe25": "0",
"searchTypeInstant": "0",
"searchTypeDrug": "0",
"searchTypeSelf25": "0",
"searchTypePost": "0",
"searchTypeATM": "0",
"searchTypeWithdrawal": "0",
"searchTypeTaxrefund": "0",
"searchTypeSmartAtm": "0",
"searchTypeSelfCookingUtensils": "0",
"searchTypeDeliveryService": "0",
"searchTypeParcelService": "0",
"searchTypePotatoes": "0",
"searchTypeCardiacDefi": "0",
"searchTypeFishShapedBun": "0",}
with requests.Session() as s:
    r = s.get("http://gs25.gsretail.com/gscvs/ko/store-services/locations")
    bs = BS(r.text)
    token = bs.find("input", {'name': "CSRFToken"})['value']
    #print(bs.find("input", {'name': "CSRFToken"}))
    target = f"http://gs25.gsretail.com/gscvs/ko/store-services/locationList?CSRFToken={token}"
    r2 = s.post(target, data=payload)




pd.DataFrame(eval(r2.json())['results'])





payload = {"pageNum": "1",
"pageSize": "5",
"searchShopName": "",
"searchSido": "",
"searchGugun": "",
"searchDong": "",
"searchType": "",
"searchTypeService": "0",
"searchTypeToto": "0",
"searchTypeCafe25": "0",
"searchTypeInstant": "0",
"searchTypeDrug": "0",
"searchTypeSelf25": "0",
"searchTypePost": "0",
"searchTypeATM": "0",
"searchTypeWithdrawal": "0",
"searchTypeTaxrefund": "0",
"searchTypeSmartAtm": "0",
"searchTypeSelfCookingUtensils": "0",
"searchTypeDeliveryService": "0",
"searchTypeParcelService": "0",
"searchTypePotatoes": "0",
"searchTypeCardiacDefi": "0",
"searchTypeFishShapedBun": "0",}
total = []
with requests.Session() as s:
    r = s.get("http://gs25.gsretail.com/gscvs/ko/store-services/locations")
    bs = BS(r.text)
    token = bs.find("input", {'name': "CSRFToken"})['value']
    #print(bs.find("input", {'name': "CSRFToken"}))
    target = f"http://gs25.gsretail.com/gscvs/ko/store-services/locationList?CSRFToken={token}"
    for x in range(1,3):
        if x % 10 == 0: print(x)
        payload['pageNum'] = x 
        r2 = s.post(target, data=payload)
        total.append( pd.DataFrame(eval(r2.json())['results']))


gs25.reset_index(drop=True, inplace=True)







